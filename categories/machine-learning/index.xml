<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on JR&#39;s tech blog</title>
    <link>http://jrusev.github.io/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on JR&#39;s tech blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Dec 2015 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://jrusev.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Hacking MNIST in 30 lines of Python</title>
      <link>http://jrusev.github.io/post/hacking-mnist/</link>
      <pubDate>Tue, 29 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>http://jrusev.github.io/post/hacking-mnist/</guid>
      <description>

&lt;p&gt;In order to better understand neural networks, I wanted to see one implemented
with a minimal amount of code. I quickly found a neural network in
&lt;a href=&#34;http://iamtrask.github.io/2015/07/12/basic-python-network/&#34;&gt;11 lines of Python&lt;/a&gt;.
It&amp;rsquo;s a great little piece of code that learns the XOR function and shows the
backpropagation in action. I felt however, that I needed a slightly more complex
example - a neural net that could recognize handwritten digits from the
&lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;MNIST dataset&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jrusev.github.io/img/mnist_digits.png&#34; alt=&#34;MNIST Digits&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Michael Nielsen&amp;rsquo;s
&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;great book&lt;/a&gt; on neural networks is
built upon just this classification problem, but the simplest implementation is
&lt;a href=&#34;https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py&#34;&gt;140 lines&lt;/a&gt;
of code (or 74 without the comments). So I took that code and refactored it down
to 30 lines. Here it is (GitHub
&lt;a href=&#34;https://github.com/jrusev/simple-neural-networks/blob/master/src/mlp_numpy.py&#34;&gt;repo&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import mnist

def feed_forward(X, weights):
    a = [X]
    for w in weights:
        a.append(sigmoid(a[-1].dot(w)))
    return a

def grads(X, Y, weights):
    grads = np.empty_like(weights)
    a = feed_forward(X, weights)
    delta = a[-1] - Y # cross-entropy
    grads[-1] = np.dot(a[-2].T, delta)
    for i in xrange(len(a)-2, 0, -1):
        delta = np.dot(delta, weights[i].T) * d_sigmoid(a[i])
        grads[i-1] = np.dot(a[i-1].T, delta)
    return grads / len(X)

sigmoid = lambda x: 1 / (1 + np.exp(-x))
d_sigmoid = lambda y: y * (1 - y)

(trX, trY), _, (teX, teY) = mnist.load_data(one_hot=True)

weights = [
    np.random.randn(784, 100) / np.sqrt(784),
    np.random.randn(100, 10) / np.sqrt(100)]
num_epochs, batch_size, learn_rate = 30, 10, 0.2

for i in xrange(num_epochs):
    for j in xrange(0, len(trX), batch_size):
        X, Y = trX[j:j+batch_size], trY[j:j+batch_size]
        weights -= learn_rate * grads(X, Y, weights)
    prediction = np.argmax(feed_forward(teX, weights)[-1], axis=1)
    print i, np.mean(prediction == np.argmax(teY, axis=1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you run this code, you will get to about 97.8% accuracy on the test set.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://jrusev.github.io/img/nn_training.png&#34; alt=&#34;Neural Network Training&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The algorithm is practically the same as the original implementation. The net
has one hidden layer with 100 neurons and uses mini batch gradient descent
to learn the weights. It has two dependencies - &lt;code&gt;numpy&lt;/code&gt;, which handles vector
and matrix operations, and &lt;code&gt;mnist&lt;/code&gt; which is a simple
&lt;a href=&#34;https://github.com/jrusev/simple-neural-networks/blob/master/src/mnist.py&#34;&gt;script&lt;/a&gt;
that downloads the MNIST data and loads it to memory.&lt;/p&gt;

&lt;p&gt;The major difference is that instead of loops I use matrix operations, which
means you can process the entire mini batch as a matrix instead of a single
training example. As a result, the network is about 5 times faster than the
original code, while you can still see what&amp;rsquo;s going on under the hood. I&amp;rsquo;ve also
dropped the bias terms because in this problem they do not considerably improve
accuracy.&lt;/p&gt;

&lt;h3 id=&#34;implementation-in-a-class:2f5fc919e238dac1556de59dd7783f3d&#34;&gt;Implementation in a class&lt;/h3&gt;

&lt;p&gt;I also implemented the same network in a class where I added the bias terms and
also shuffle the training data each epoch. The code is available
&lt;a href=&#34;https://github.com/jrusev/simple-neural-networks/blob/master/src/network.py&#34;&gt;here&lt;/a&gt;.
You can run it and plot the accuracy after each epoch like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import network
import mnist
import matplotlib.pyplot as plt

(trX, trY), _, (teX, teY) = mnist.load_data(one_hot=True)

net = network.Network([784, 100, 10])
acc = net.train(trX, trY, teX, teY, num_epochs=30, batch_size=10, learn_rate=0.2)

plt.plot(acc)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;implementation-with-theano:2f5fc919e238dac1556de59dd7783f3d&#34;&gt;Implementation with Theano&lt;/h3&gt;

&lt;p&gt;Once we understand how backpropagation works, we can hide it by using a library
such as Theano or Torch. So I implemented the exact same network as above using
Theano also in 30 lines of code. Theano can calculate the gradients
automatically after we compose the network graph. It&amp;rsquo;s also about 2 times faster
than the numpy-only implementation. Here is the code
(&lt;a href=&#34;https://github.com/jrusev/simple-neural-networks/blob/master/src/mlp_theano.py&#34;&gt;source&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import theano
import theano.tensor as T
import numpy as np
import mnist

def init_weights(n_in, n_out):
    weights = np.random.randn(n_in, n_out) / np.sqrt(n_in)
    return theano.shared(np.asarray(weights, dtype=theano.config.floatX))

def feed_forward(X, w_h, w_o):
    h = T.nnet.sigmoid(T.dot(X, w_h))
    return T.nnet.softmax(T.dot(h, w_o))

(trX, trY), _, (teX, teY) = mnist.load_data(one_hot=True)

w_h, w_o = init_weights(784, 100), init_weights(100, 10)
num_epochs, batch_size, eta = 30, 10, 0.2

X, Y = T.fmatrices(&#39;X&#39;, &#39;Y&#39;)
out = feed_forward(X, w_h, w_o)
cost = T.nnet.categorical_crossentropy(out, Y).mean()

weights = [w_h, w_o]
grads = T.grad(cost=cost, wrt=weights)
apply_grads = theano.function(
    inputs=[X, Y],
    updates=[[w, w - g * eta] for w, g in zip(weights, grads)],
    allow_input_downcast=True)
predict = theano.function(inputs=[X], outputs=T.argmax(out, axis=1))

for i in range(num_epochs):
    for j in xrange(0, len(trX), batch_size):
        apply_grads(trX[j:j+batch_size], trY[j:j+batch_size])
    print i, np.mean(predict(teX) == np.argmax(teY, axis=1))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>